{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-autotime\n",
    "%load_ext autotime\n",
    "import numpy as np\n",
    "x = np.array([[0,1],[2,6],[3,8]]) # x1, x2\n",
    "y = np.array([1,1,4])\n",
    "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
    "def cost_function(theta, x, y, N):\n",
    "   y_hat = x.dot(theta)\n",
    "   c = (1/(2*N)) * np.sum((y_hat-y)**2)\n",
    "   return c\n",
    "def mini_batch_gradient_descent(alpha, x, y, batch_size=1, ep=0.001, max_iter=10000):\n",
    "   converged = False\n",
    "   iter = 0\n",
    "   N = x.shape[0]  # number of samples\n",
    "   print(\"Num of data = \", N)\n",
    "   # initial theta\n",
    "   theta = np.random.random((x.shape[1], 1))\n",
    "   print(\"Init theta.shape = \", theta.shape)\n",
    "   # total error, J(theta)\n",
    "   J = cost_function(theta, x, y, N)\n",
    "   print(\"First J = \", J)\n",
    "   # Iterate Loop\n",
    "   while not converged and iter < max_iter:\n",
    "       for i in range(0, N, batch_size):\n",
    "           # Create mini-batch\n",
    "           x_batch = x[i:i+batch_size]\n",
    "           y_batch = y[i:i+batch_size]\n",
    "           # Predict\n",
    "           y_hat = x_batch.dot(theta)\n",
    "           # Calculate gradient\n",
    "           diff = y_hat - y_batch\n",
    "           grad = x_batch.T.dot(diff)\n",
    "           # Update theta\n",
    "           theta = theta - alpha * (1/batch_size) * grad\n",
    "           assert theta.shape == (3, 1)  # Ensure theta's shape remains the same\n",
    "       # Calculate new cost\n",
    "       J2 = cost_function(theta, x, y, N)\n",
    "       if abs(J-J2) <= ep:\n",
    "           print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
    "           converged = True\n",
    "       J = J2  # update error\n",
    "       iter += 1  # update iter\n",
    "       if iter == max_iter:\n",
    "           print('       Max iterations exceeded!')\n",
    "           converged = True\n",
    "   return theta\n",
    "if __name__ == '__main__':\n",
    "   print(\"start main\")\n",
    "   print(x_b.shape)\n",
    "   y = y.reshape(-1, 1)\n",
    "   print(y.shape)\n",
    "   alpha = 0.01  # learning rate\n",
    "   # Training process\n",
    "   theta = mini_batch_gradient_descent(alpha, x_b, y, batch_size=1, ep=0.000000000001, max_iter=1000000)\n",
    "   print(\"Theta = \", theta)\n",
    "   # predict trained x\n",
    "   xtest = np.array([[4, 9]])\n",
    "   xtest_b = np.c_[np.ones((xtest.shape[0], 1)), xtest]\n",
    "   y_p = xtest_b.dot(theta)\n",
    "   print(\"y predict = \", y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-autotime\n",
    "%load_ext autotime\n",
    "import numpy as np\n",
    "x = np.array([[0,1],[2,6],[3,8]]) # x1, x2\n",
    "y = np.array([1,1,4])\n",
    "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
    "def cost_function(theta, x, y, N):\n",
    "   y_hat = x.dot(theta)\n",
    "   c = (1/(2*N)) * np.sum((y_hat-y)**2)\n",
    "   return c\n",
    "def mini_batch_gradient_descent(alpha, x, y, batch_size=2, ep=0.001, max_iter=10000):\n",
    "   converged = False\n",
    "   iter = 0\n",
    "   N = x.shape[0]  # number of samples\n",
    "   print(\"Num of data = \", N)\n",
    "   # initial theta\n",
    "   theta = np.random.random((x.shape[1], 1))\n",
    "   print(\"Init theta.shape = \", theta.shape)\n",
    "   # total error, J(theta)\n",
    "   J = cost_function(theta, x, y, N)\n",
    "   print(\"First J = \", J)\n",
    "   # Iterate Loop\n",
    "   while not converged and iter < max_iter:\n",
    "       for i in range(0, N, batch_size):\n",
    "           # Create mini-batch\n",
    "           x_batch = x[i:i+batch_size]\n",
    "           y_batch = y[i:i+batch_size]\n",
    "           # Predict\n",
    "           y_hat = x_batch.dot(theta)\n",
    "           # Calculate gradient\n",
    "           diff = y_hat - y_batch\n",
    "           grad = x_batch.T.dot(diff)\n",
    "           # Update theta\n",
    "           theta = theta - alpha * (1/batch_size) * grad\n",
    "           assert theta.shape == (3, 1)  # Ensure theta's shape remains the same\n",
    "       # Calculate new cost\n",
    "       J2 = cost_function(theta, x, y, N)\n",
    "       if abs(J-J2) <= ep:\n",
    "           print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
    "           converged = True\n",
    "       J = J2  # update error\n",
    "       iter += 1  # update iter\n",
    "       if iter == max_iter:\n",
    "           print('       Max iterations exceeded!')\n",
    "           converged = True\n",
    "   return theta\n",
    "if __name__ == '__main__':\n",
    "   print(\"start main\")\n",
    "   print(x_b.shape)\n",
    "   y = y.reshape(-1, 1)\n",
    "   print(y.shape)\n",
    "   alpha = 0.01  # learning rate\n",
    "   # Training process\n",
    "   theta = mini_batch_gradient_descent(alpha, x_b, y, batch_size=2, ep=0.000000000001, max_iter=1000000)\n",
    "   print(\"Theta = \", theta)\n",
    "   # predict trained x\n",
    "   xtest = np.array([[4, 9]])\n",
    "   xtest_b = np.c_[np.ones((xtest.shape[0], 1)), xtest]\n",
    "   y_p = xtest_b.dot(theta)\n",
    "   print(\"y predict = \", y_p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
